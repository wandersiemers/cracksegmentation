{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pavementcracks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "332075e5de88426da4336b5143c92608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dd9efdb3b96540c29930f10a99d1a465",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_252d3ade33124ba9a7be1a8d5a3dcc57",
              "IPY_MODEL_0f53cc85a3c0444eb1c9afe3a32f4690"
            ]
          }
        },
        "dd9efdb3b96540c29930f10a99d1a465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "252d3ade33124ba9a7be1a8d5a3dcc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c9922fc5d11747a5956c857d5f1fb609",
            "_dom_classes": [],
            "description": "Epoch 1:   3%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1040,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 32,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_caf24766b9044933a4baeafabe2a8b85"
          }
        },
        "0f53cc85a3c0444eb1c9afe3a32f4690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0f19155e53cf49c7a59ac29be5778ad0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32/1040 [00:01&lt;00:34, 29.35it/s, loss=0.27580]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2c1a6a15d72540da9936608d9c520e6e"
          }
        },
        "c9922fc5d11747a5956c857d5f1fb609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "caf24766b9044933a4baeafabe2a8b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f19155e53cf49c7a59ac29be5778ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2c1a6a15d72540da9936608d9c520e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT9C_gmmRlQx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# class DoubleConv(nn.Module):\n",
        "#     \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "#     def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "#         super().__init__()\n",
        "#         if not mid_channels:\n",
        "#             mid_channels = out_channels\n",
        "#         self.double_conv = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, mid_channels, kernel_size=2, padding=1),\n",
        "#             nn.BatchNorm2d(mid_channels),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(mid_channels, out_channels, kernel_size=2, padding=1),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.double_conv(x)\n",
        "\n",
        "# class Up(nn.Module):\n",
        "#     \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "#     def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "#         if bilinear:\n",
        "#             self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "#             self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "#         else:\n",
        "#             self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "#             self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "#     def forward(self, x1, x2):\n",
        "#         x1 = self.up(x1)\n",
        "#         # input is CHW\n",
        "#         diffY = x2.size()[2] - x1.size()[2]\n",
        "#         diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "#         x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "#                         diffY // 2, diffY - diffY // 2])\n",
        "#         # if you have padding issues, see\n",
        "#         # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "#         # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "#         x = torch.cat([x2, x1], dim=1)\n",
        "#         return self.conv(x)\n",
        "\n",
        "# Source\n",
        "# https://github.com/nyoki-mtl/pytorch-segmentation/blob/master/src/models/scse.py\n",
        "class SCSEBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel // reduction)),\n",
        "                                                nn.ReLU(inplace=True),\n",
        "                                                nn.Linear(int(channel // reduction), channel))\n",
        "        self.spatial_se = nn.Conv2d(channel, 1, kernel_size=1,\n",
        "                                    stride=1, padding=0, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bahs, chs, _, _ = x.size()\n",
        "\n",
        "        # Returns a new tensor with the same data as the self tensor but of a different size.\n",
        "        chn_se = self.avg_pool(x).view(bahs, chs)\n",
        "        chn_se = torch.sigmoid(self.channel_excitation(chn_se).view(bahs, chs, 1, 1))\n",
        "        chn_se = torch.mul(x, chn_se)\n",
        "\n",
        "        spa_se = torch.sigmoid(self.spatial_se(x))\n",
        "        spa_se = torch.mul(x, spa_se)\n",
        "        return torch.add(chn_se, 1, spa_se)\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6heYSJv-kZ93"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1mIP4uFoRDG"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "from torchvision import models\n",
        "import torchvision\n",
        "\n",
        "input_size = (448, 448)\n",
        "\n",
        "custom_decoder = True\n",
        "\n",
        "class Interpolate(nn.Module):\n",
        "    def __init__(self, size=None, scale_factor=None, mode='nearest', align_corners=False):\n",
        "        super(Interpolate, self).__init__()\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.size = size\n",
        "        self.mode = mode\n",
        "        self.scale_factor = scale_factor\n",
        "        self.align_corners = align_corners\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.interp(x, size=self.size, scale_factor=self.scale_factor,\n",
        "                        mode=self.mode, align_corners=self.align_corners)\n",
        "        return x\n",
        "\n",
        "def conv3x3(in_, out):\n",
        "    return nn.Conv2d(in_, out, 3, padding=1)\n",
        "\n",
        "\n",
        "class ConvRelu(nn.Module):\n",
        "    def __init__(self, in_, out):\n",
        "        super().__init__()\n",
        "        self.conv = conv3x3(in_, out)\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderBlockV2(nn.Module):\n",
        "    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n",
        "        super(DecoderBlockV2, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        if is_deconv:\n",
        "            \"\"\"\n",
        "                Paramaters for Deconvolution were chosen to avoid artifacts, following\n",
        "                link https://distill.pub/2016/deconv-checkerboard/\n",
        "            \"\"\"\n",
        "\n",
        "            self.block = nn.Sequential(\n",
        "                ConvRelu(in_channels, middle_channels),\n",
        "                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n",
        "                                  padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "        else:\n",
        "            self.block = nn.Sequential(\n",
        "                Interpolate(scale_factor=2, mode='bilinear'),\n",
        "                ConvRelu(in_channels, middle_channels),\n",
        "                ConvRelu(middle_channels, out_channels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class MyDecoderBlockV2(nn.Module):\n",
        "    \"\"\"\n",
        "    All the other decoder blocks\n",
        "    Performs Relu, BN, SCSE, 2x2 Transpose conv\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, is_deconv=True):\n",
        "          super(MyDecoderBlockV2, self).__init__()\n",
        "          self.in_channels = in_channels\n",
        "\n",
        "          self.block = nn.Sequential(\n",
        "              nn.ReLU(),\n",
        "              nn.BatchNorm2d(in_channels),\n",
        "              SCSEBlock(in_channels),\n",
        "              nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "          )\n",
        "\n",
        "    def forward(self, x):\n",
        "          return self.block(x)\n",
        "\n",
        "class UNet16(nn.Module):\n",
        "    def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):\n",
        "        \"\"\"\n",
        "        :param num_classes:\n",
        "        :param num_filters:\n",
        "        :param pretrained:\n",
        "            False - no pre-trained network used\n",
        "            True - encoder pre-trained with VGG16\n",
        "        :is_deconv:\n",
        "            False: bilinear interpolation is used in decoder\n",
        "            True: deconvolution is used in decoder\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        #print(torchvision.models.vgg16(pretrained=pretrained))\n",
        "\n",
        "        print(\"creating encoder\")\n",
        "\n",
        "        # >>>\n",
        "        self.encoder = torchvision.models.vgg16(pretrained=pretrained).features\n",
        "        #self.encoder = torchvision.models.resnet34(pretrained=pretrained).features\n",
        "\n",
        "        print(\"creating relu\")\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv1 = nn.Sequential(self.encoder[0],\n",
        "                                   self.relu,\n",
        "                                   self.encoder[2],\n",
        "                                   self.relu)\n",
        "\n",
        "        self.conv2 = nn.Sequential(self.encoder[5],\n",
        "                                   self.relu,\n",
        "                                   self.encoder[7],\n",
        "                                   self.relu)\n",
        "\n",
        "        self.conv3 = nn.Sequential(self.encoder[10],\n",
        "                                   self.relu,\n",
        "                                   self.encoder[12],\n",
        "                                   self.relu,\n",
        "                                   self.encoder[14],\n",
        "                                   self.relu)\n",
        "\n",
        "        self.conv4 = nn.Sequential(self.encoder[17],\n",
        "                                   self.relu,\n",
        "                                   self.encoder[19],\n",
        "                                   self.relu,\n",
        "                                   self.encoder[21],\n",
        "                                   self.relu)\n",
        "\n",
        "        self.conv5 = nn.Sequential(self.encoder[24],\n",
        "                                   self.relu,\n",
        "                                   self.encoder[26],\n",
        "                                   self.relu,\n",
        "                                   self.encoder[28],\n",
        "                                   self.relu)\n",
        "        \n",
        "        print(\"creating center\")\n",
        "\n",
        "        self.center = DecoderBlockV2(512, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
        "\n",
        "        print(\"creating decoder\")\n",
        "\n",
        "        self.dec5 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
        "        self.dec4 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
        "        self.dec3 = DecoderBlockV2(256 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)\n",
        "        self.dec2 = DecoderBlockV2(128 + num_filters * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
        "        self.dec1 = ConvRelu(64 + num_filters, num_filters)\n",
        "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv1(x)\n",
        "        conv2 = self.conv2(self.pool(conv1))\n",
        "        conv3 = self.conv3(self.pool(conv2))\n",
        "        conv4 = self.conv4(self.pool(conv3))\n",
        "        conv5 = self.conv5(self.pool(conv4))\n",
        "\n",
        "        center = self.center(self.pool(conv5))\n",
        "\n",
        "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
        "\n",
        "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
        "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
        "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
        "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
        "\n",
        "        if self.num_classes > 1:\n",
        "            x_out = F.log_softmax(self.final(dec1), dim=1)\n",
        "        else:\n",
        "            x_out = self.final(dec1)\n",
        "            #x_out = F.sigmoid(x_out)\n",
        "\n",
        "        return x_out\n",
        "\n",
        "class UNetResNet(nn.Module):\n",
        "    \"\"\"PyTorch U-Net model using ResNet(34, 101 or 152) encoder.\n",
        "    UNet: https://arxiv.org/abs/1505.04597\n",
        "    ResNet: https://arxiv.org/abs/1512.03385\n",
        "    Proposed by Alexander Buslaev: https://www.linkedin.com/in/al-buslaev/\n",
        "    Args:\n",
        "            encoder_depth (int): Depth of a ResNet encoder (34, 101 or 152).\n",
        "            num_classes (int): Number of output classes.\n",
        "            num_filters (int, optional): Number of filters in the last layer of decoder. Defaults to 32.\n",
        "            dropout_2d (float, optional): Probability factor of dropout layer before output layer. Defaults to 0.2.\n",
        "            pretrained (bool, optional):\n",
        "                False - no pre-trained weights are being used.\n",
        "                True  - ResNet encoder is pre-trained on ImageNet.\n",
        "                Defaults to False.\n",
        "            is_deconv (bool, optional):\n",
        "                False: bilinear interpolation is used in decoder.\n",
        "                True: deconvolution is used in decoder.\n",
        "                Defaults to False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_depth, num_classes, num_filters=32, dropout_2d=0.2,\n",
        "                 pretrained=False, is_deconv=False):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_2d = dropout_2d\n",
        "\n",
        "        if encoder_depth == 34:\n",
        "            self.encoder = torchvision.models.resnet34(pretrained=pretrained)\n",
        "            bottom_channel_nr = 512\n",
        "        elif encoder_depth == 101:\n",
        "            self.encoder = torchvision.models.resnet101(pretrained=pretrained)\n",
        "            bottom_channel_nr = 2048\n",
        "        elif encoder_depth == 152:\n",
        "            self.encoder = torchvision.models.resnet152(pretrained=pretrained)\n",
        "            bottom_channel_nr = 2048\n",
        "        else:\n",
        "            raise NotImplementedError('only 34, 101, 152 version of Resnet are implemented')\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if custom_decoder:\n",
        "            self.do_conv_1 = nn.Conv2d(64, 128, kernel_size=1, stride=1)\n",
        "            self.do_conv_2 = nn.Conv2d(64, 128, kernel_size=1, stride=1)\n",
        "            self.do_conv_3 = nn.Conv2d(128, 128, kernel_size=1, stride=1)\n",
        "            self.do_conv_4 = nn.Conv2d(256, 128, kernel_size=1, stride=1)\n",
        "            self.do_conv_5 = nn.Conv2d(512, 512, kernel_size=1, stride=1) \n",
        "\n",
        "            # Encoder layers\n",
        "            self.conv1 = nn.Sequential(self.encoder.conv1,\n",
        "                                      self.encoder.bn1,\n",
        "                                      self.encoder.relu)\n",
        "            self.conv2 = self.encoder.layer1\n",
        "            self.conv3 = self.encoder.layer2\n",
        "            self.conv4 = self.encoder.layer3\n",
        "            self.conv5 = self.encoder.layer4\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(self.encoder.conv1,\n",
        "                                      self.encoder.bn1,\n",
        "                                      self.encoder.relu,\n",
        "                                      self.pool)\n",
        "            self.conv2 = self.encoder.layer1\n",
        "            self.conv3 = self.encoder.layer2\n",
        "            self.conv4 = self.encoder.layer3\n",
        "            self.conv5 = self.encoder.layer4\n",
        "\n",
        "        \n",
        "        # >>>\n",
        "        if custom_decoder:\n",
        "            self.dec4 = nn.ConvTranspose2d(512, 128, kernel_size=2, stride=2)\n",
        "            self.dec3 = MyDecoderBlockV2(256, 128)\n",
        "            self.dec2 = MyDecoderBlockV2(256, 128)\n",
        "            self.dec1 = MyDecoderBlockV2(256, 128)\n",
        "            self.dec0 = MyDecoderBlockV2(256, 1)\n",
        "\n",
        "        else:\n",
        "            self.center = DecoderBlockV2(bottom_channel_nr, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
        "            self.dec5 = DecoderBlockV2(bottom_channel_nr + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
        "            self.dec4 = DecoderBlockV2(bottom_channel_nr // 2 + num_filters * 8, num_filters * 8 * 2, num_filters * 8,\n",
        "                                      is_deconv)\n",
        "            self.dec3 = DecoderBlockV2(bottom_channel_nr // 4 + num_filters * 8, num_filters * 4 * 2, num_filters * 2,\n",
        "                                      is_deconv)\n",
        "            self.dec2 = DecoderBlockV2(bottom_channel_nr // 8 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2,\n",
        "                                      is_deconv)\n",
        "            self.dec1 = DecoderBlockV2(num_filters * 2 * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
        "            self.dec0 = ConvRelu(num_filters, num_filters)\n",
        "            self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if custom_decoder:\n",
        "            conv1 = self.conv1(x)\n",
        "            conv1_pool = self.pool(conv1)\n",
        "            conv2 = self.conv2(conv1_pool)\n",
        "            conv3 = self.conv3(conv2)\n",
        "            conv4 = self.conv4(conv3)\n",
        "            conv5 = self.conv5(conv4)\n",
        "\n",
        "            # Apply 1x1 conv before concattenating\n",
        "            conv1 = self.do_conv_1(conv1)\n",
        "            conv2 = self.do_conv_2(conv2)\n",
        "            conv3 = self.do_conv_3(conv3)\n",
        "            conv4 = self.do_conv_4(conv4)\n",
        "            conv5 = self.do_conv_5(conv5)\n",
        "\n",
        "            # Upsampling\n",
        "            dec4 = self.dec4(conv5)\n",
        "            dec3 = self.dec3(torch.cat([dec4, conv4], 1))\n",
        "            dec2 = self.dec2(torch.cat([dec3, conv3], 1))\n",
        "            dec1 = self.dec1(torch.cat([dec2, conv2], 1))\n",
        "            dec0 = self.dec0(torch.cat([dec1, conv1], 1))\n",
        "\n",
        "            return dec0\n",
        "        else:\n",
        "            conv1 = self.conv1(x)\n",
        "            conv2 = self.conv2(conv1)\n",
        "            conv3 = self.conv3(conv2)\n",
        "            conv4 = self.conv4(conv3)\n",
        "            conv5 = self.conv5(conv4)\n",
        "\n",
        "            pool = self.pool(conv5)\n",
        "            center = self.center(pool)\n",
        "\n",
        "            dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
        "\n",
        "            dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
        "            dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
        "            dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
        "            dec1 = self.dec1(dec2)\n",
        "            dec0 = self.dec0(dec1)\n",
        "\n",
        "            return self.final(F.dropout2d(dec0, p=self.dropout_2d))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e6ojHnECxKs",
        "outputId": "e9723a25-088f-4fc1-9720-dab9747911a1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLm6NHbchllT"
      },
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def map_location():\n",
        "  if torch.cuda.is_available():\n",
        "    print(\"cuda is available, using gpu\")\n",
        "    return torch.device(\"cuda:0\")\n",
        "    #return torch.device(\"cpu\")\n",
        "  else:\n",
        "    print(\"cuda is unavailable, using cpu\")\n",
        "    return torch.device(\"cpu\")\n",
        "    \n",
        "\n",
        "def cuda(x):\n",
        "    return x.cuda(non_blocking=True) if torch.cuda.is_available() else x\n",
        "\n",
        "def write_event(log, step, **data):\n",
        "    data['step'] = step\n",
        "    data['dt'] = datetime.now().isoformat()\n",
        "    log.write(json.dumps(data, sort_keys=True))\n",
        "    log.write('\\n')\n",
        "    log.flush()\n",
        "\n",
        "def check_crop_size(image_height, image_width):\n",
        "    \"\"\"Checks if image size divisible by 32.\n",
        "    Args:\n",
        "        image_height:\n",
        "        image_width:\n",
        "    Returns:\n",
        "        True if both height and width divisible by 32 and False otherwise.\n",
        "    \"\"\"\n",
        "    return image_height % 32 == 0 and image_width % 32 == 0\n",
        "\n",
        "def create_model(device, type ='vgg16'):\n",
        "    assert type == 'vgg16' or type == 'resnet101'\n",
        "    if type == 'vgg16':\n",
        "        model = UNet16(pretrained=True)\n",
        "    elif type == 'resnet101':\n",
        "        model = UNetResNet(pretrained=True, encoder_depth=101, num_classes=1)\n",
        "    else:\n",
        "        assert False\n",
        "    model.eval()\n",
        "    return model.to(device)\n",
        "\n",
        "def load_unet_vgg16(model_path):\n",
        "    model = UNet16(pretrained=True)\n",
        "    print(\"loading model\")\n",
        "    checkpoint = torch.load(model_path, map_location=map_location())\n",
        "    print(\"loaded model\")\n",
        "    if 'model' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "    elif 'state_dict' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['check_point'])\n",
        "    else:\n",
        "        raise Exception('undefind model format')\n",
        "\n",
        "\n",
        "    if (torch.cuda.is_available):\n",
        "      model.cuda()\n",
        "    print(\"evalling\")\n",
        "    model.eval()\n",
        "    print(\"evalling done\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_unet_resnet_101(model_path):\n",
        "    model = UNetResNet(pretrained=True, encoder_depth=101, num_classes=1)\n",
        "    checkpoint = torch.load(model_path, map_location())\n",
        "    if 'model' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "    elif 'state_dict' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['check_point'])\n",
        "    else:\n",
        "        raise Exception('undefind model format')\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_unet_resnet_34(model_path):\n",
        "    model = UNetResNet(pretrained=True, encoder_depth=34, num_classes=1)\n",
        "    checkpoint = torch.load(model_path, map_location())\n",
        "    if 'model' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "    elif 'state_dict' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['check_point'])\n",
        "    else:\n",
        "        raise Exception('undefind model format')\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "def train(args, model, criterion, train_loader, valid_loader, validation, init_optimizer, n_epochs=None, fold=None,\n",
        "          num_classes=None):\n",
        "    lr = args.lr\n",
        "    n_epochs = n_epochs or args.n_epochs\n",
        "    optimizer = init_optimizer(lr)\n",
        "\n",
        "    root = Path(args.model_path)\n",
        "    model_path = root / 'model_{fold}.pt'.format(fold=fold)\n",
        "    if model_path.exists():\n",
        "        state = torch.load(str(model_path))\n",
        "        epoch = state['epoch']\n",
        "        step = state['step']\n",
        "        model.load_state_dict(state['model'])\n",
        "        print('Restored model, epoch {}, step {:,}'.format(epoch, step))\n",
        "    else:\n",
        "        epoch = 1\n",
        "        step = 0\n",
        "\n",
        "    save = lambda ep: torch.save({\n",
        "        'model': model.state_dict(),\n",
        "        'epoch': ep,\n",
        "        'step': step,\n",
        "    }, str(model_path))\n",
        "\n",
        "    report_each = 10\n",
        "    log = root.joinpath('train_{fold}.log'.format(fold=fold)).open('at', encoding='utf8')\n",
        "    valid_losses = []\n",
        "    for epoch in range(epoch, n_epochs + 1):\n",
        "        model.train()\n",
        "        random.seed()\n",
        "        tq = tqdm.tqdm(total=(len(train_loader) * args.batch_size))\n",
        "        tq.set_description('Epoch {}, lr {}'.format(epoch, lr))\n",
        "        losses = []\n",
        "        tl = train_loader\n",
        "        try:\n",
        "            mean_loss = 0\n",
        "            for i, (inputs, targets) in enumerate(tl):\n",
        "                inputs = cuda(inputs)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    targets = cuda(targets)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                #print(outputs.shape, targets.shape)\n",
        "                loss = criterion(outputs, targets)\n",
        "                optimizer.zero_grad()\n",
        "                batch_size = inputs.size(0)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                step += 1\n",
        "                tq.update(batch_size)\n",
        "                losses.append(loss.item())\n",
        "                mean_loss = np.mean(losses[-report_each:])\n",
        "                tq.set_postfix(loss='{:.5f}'.format(mean_loss))\n",
        "                if i and i % report_each == 0:\n",
        "                    write_event(log, step, loss=mean_loss)\n",
        "            write_event(log, step, loss=mean_loss)\n",
        "            tq.close()\n",
        "            save(epoch + 1)\n",
        "            valid_metrics = validation(model, criterion, valid_loader, num_classes)\n",
        "            write_event(log, step, **valid_metrics)\n",
        "            valid_loss = valid_metrics['valid_loss']\n",
        "            valid_losses.append(valid_loss)\n",
        "        except KeyboardInterrupt:\n",
        "            tq.close()\n",
        "            print('Ctrl+C, saving snapshot')\n",
        "            save(epoch)\n",
        "            print('done.')\n",
        "            return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKRoUjPLLQhp"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ImgDataSet(Dataset):\n",
        "    def __init__(self, img_dir, img_fnames, img_transform, mask_dir, mask_fnames, mask_transform):\n",
        "        self.img_dir = img_dir\n",
        "        self.img_fnames = img_fnames\n",
        "        self.img_transform = img_transform\n",
        "\n",
        "        self.mask_dir = mask_dir\n",
        "        self.mask_fnames = mask_fnames\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        self.seed = np.random.randint(2147483647)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        fname = self.img_fnames[i]\n",
        "        fpath = os.path.join(self.img_dir, fname)\n",
        "        img = Image.open(fpath)\n",
        "        if self.img_transform is not None:\n",
        "            random.seed(self.seed)\n",
        "            img = self.img_transform(img)\n",
        "            #print('image shape', img.shape)\n",
        "\n",
        "        mname = self.mask_fnames[i]\n",
        "        mpath = os.path.join(self.mask_dir, mname)\n",
        "        mask = Image.open(mpath)\n",
        "        #print('khanh1', np.min(test[:]), np.max(test[:]))\n",
        "        if self.mask_transform is not None:\n",
        "            mask = self.mask_transform(mask)\n",
        "            #print('mask shape', mask.shape)\n",
        "            #print('khanh2', np.min(test[:]), np.max(test[:]))\n",
        "\n",
        "        return img, mask #torch.from_numpy(np.array(mask, dtype=np.int64))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_fnames)\n",
        "\n",
        "\n",
        "class ImgDataSetJoint(Dataset):\n",
        "    def __init__(self, img_dir, img_fnames, joint_transform, mask_dir, mask_fnames, img_transform = None, mask_transform = None):\n",
        "        self.joint_transform = joint_transform\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.img_fnames = img_fnames\n",
        "        self.img_transform = img_transform\n",
        "\n",
        "        self.mask_dir = mask_dir\n",
        "        self.mask_fnames = mask_fnames\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        self.seed = np.random.randint(2147483647)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        fname = self.img_fnames[i]\n",
        "        fpath = os.path.join(self.img_dir, fname)\n",
        "        img = Image.open(fpath)\n",
        "\n",
        "        mname = self.mask_fnames[i]\n",
        "        mpath = os.path.join(self.mask_dir, mname)\n",
        "        mask = Image.open(mpath)\n",
        "\n",
        "        if self.joint_transform is not None:\n",
        "            img, mask = self.joint_transform([img, mask])\n",
        "\n",
        "        #debug\n",
        "        # img = np.asarray(img)\n",
        "        # mask = np.asarray(mask)\n",
        "        # plt.subplot(121)\n",
        "        # plt.imshow(img)\n",
        "        # plt.subplot(122)\n",
        "        # plt.imshow(img)\n",
        "        # plt.imshow(mask, alpha=0.4)\n",
        "        # plt.show()\n",
        "\n",
        "        if self.img_transform is not None:\n",
        "            img = self.img_transform(img)\n",
        "\n",
        "        if self.mask_transform is not None:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        return img, mask #torch.from_numpy(np.array(mask, dtype=np.int64))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_fnames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQtv87uCgHnH"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import cv2 as cv\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from os.path import join\n",
        "from PIL import Image\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "def evaluate_img(model, img, train_tfms, img_width, img_height):\n",
        "    input_width, input_height = input_size[0], input_size[1]\n",
        "\n",
        "    img_1 = cv.resize(img, (input_width, input_height), cv.INTER_AREA)\n",
        "    X = train_tfms(Image.fromarray(img_1))\n",
        "    X = Variable(X.unsqueeze(0)).cuda()  # [N, 1, H, W]\n",
        "\n",
        "    mask = model(X)\n",
        "\n",
        "    mask = F.sigmoid(mask[0, 0]).data.cpu().numpy()\n",
        "    mask = cv.resize(mask, (img_width, img_height), cv.INTER_AREA)\n",
        "    return mask\n",
        "\n",
        "def evaluate_img_patch(model, img, train_tfms, img_width, img_height):\n",
        "    input_width, input_height = input_size[0], input_size[1]\n",
        "\n",
        "    img_height, img_width, img_channels = img.shape\n",
        "\n",
        "    if img_width < input_width or img_height < input_height:\n",
        "        return evaluate_img(model, img)\n",
        "\n",
        "    stride_ratio = 0.1\n",
        "    stride = int(input_width * stride_ratio)\n",
        "\n",
        "    normalization_map = np.zeros((img_height, img_width), dtype=np.int16)\n",
        "\n",
        "    patches = []\n",
        "    patch_locs = []\n",
        "    for y in range(0, img_height - input_height + 1, stride):\n",
        "        for x in range(0, img_width - input_width + 1, stride):\n",
        "            segment = img[y:y + input_height, x:x + input_width]\n",
        "            normalization_map[y:y + input_height, x:x + input_width] += 1\n",
        "            patches.append(segment)\n",
        "            patch_locs.append((x, y))\n",
        "\n",
        "    patches = np.array(patches)\n",
        "    if len(patch_locs) <= 0:\n",
        "        return None\n",
        "\n",
        "    preds = []\n",
        "    for i, patch in enumerate(patches):\n",
        "        patch_n = train_tfms(Image.fromarray(patch))\n",
        "        X = Variable(patch_n.unsqueeze(0)).cuda()  # [N, 1, H, W]\n",
        "        masks_pred = model(X)\n",
        "        mask = F.sigmoid(masks_pred[0, 0]).data.cpu().numpy()\n",
        "        preds.append(mask)\n",
        "\n",
        "    probability_map = np.zeros((img_height, img_width), dtype=float)\n",
        "    for i, response in enumerate(preds):\n",
        "        coords = patch_locs[i]\n",
        "        probability_map[coords[1]:coords[1] + input_height, coords[0]:coords[0] + input_width] += response\n",
        "\n",
        "    return probability_map\n",
        "\n",
        "def disable_axis():\n",
        "    plt.axis('off')\n",
        "    plt.gca().axes.get_xaxis().set_visible(False)\n",
        "    plt.gca().axes.get_yaxis().set_visible(False)\n",
        "    plt.gca().axes.get_xaxis().set_ticklabels([])\n",
        "    plt.gca().axes.get_yaxis().set_ticklabels([])\n",
        "\n",
        "def infer():\n",
        "    out_viz_dir = 'drive/MyDrive/synced/outviz'\n",
        "    out_pred_dir = 'drive/MyDrive/synced/outpred'\n",
        "    img_dir = 'drive/MyDrive/synced/images'\n",
        "    # >>>\n",
        "    model_type = 'resnet34'\n",
        "    # model_type = 'vgg16'\n",
        "    model_path = 'drive/MyDrive/synced/models/model_best.pt'\n",
        "    threshold = 0.2\n",
        "\n",
        "\n",
        "    if out_viz_dir != '':\n",
        "        os.makedirs(out_viz_dir, exist_ok=True)\n",
        "        for path in Path(out_viz_dir).glob('*.*'):\n",
        "            os.remove(str(path))\n",
        "\n",
        "    if out_pred_dir != '':\n",
        "        os.makedirs(out_pred_dir, exist_ok=True)\n",
        "        for path in Path(out_pred_dir).glob('*.*'):\n",
        "            os.remove(str(path))\n",
        "\n",
        "    if model_type == 'vgg16':\n",
        "        model = load_unet_vgg16(model_path)\n",
        "    elif model_type  == 'resnet101':\n",
        "        model = load_unet_resnet_101(model_path)\n",
        "    elif model_type  == 'resnet34':\n",
        "        model = load_unet_resnet_34(model_path)\n",
        "        print(model)\n",
        "    else:\n",
        "        print('undefined model name pattern')\n",
        "        exit()\n",
        "\n",
        "\n",
        "    channel_means = [0.485, 0.456, 0.406]\n",
        "    channel_stds  = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "    paths = [path for path in Path(img_dir).glob('*.*')]\n",
        "    paths = paths[0:10]\n",
        "    for path in tqdm(paths):\n",
        "        print(str(path))\n",
        "\n",
        "        train_tfms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(channel_means, channel_stds)])\n",
        "\n",
        "        img_0 = Image.open(str(path))\n",
        "        img_0 = np.asarray(img_0)\n",
        "        if len(img_0.shape) != 3:\n",
        "            print(f'incorrect image shape: {path.name}{img_0.shape}')\n",
        "            continue\n",
        "\n",
        "        img_0 = img_0[:,:,:3]\n",
        "\n",
        "        img_height, img_width, img_channels = img_0.shape\n",
        "\n",
        "        prob_map_full = evaluate_img(model, img_0, train_tfms, img_width, img_height)\n",
        "\n",
        "        if out_pred_dir != '':\n",
        "            cv.imwrite(filename=join(out_pred_dir, f'{path.stem}.jpg'), img=(prob_map_full * 255).astype(np.uint8))\n",
        "\n",
        "        if out_viz_dir != '':\n",
        "            # plt.subplot(121)\n",
        "            # plt.imshow(img_0), plt.title(f'{img_0.shape}')\n",
        "            if img_0.shape[0] > 2000 or img_0.shape[1] > 2000:\n",
        "                img_1 = cv.resize(img_0, None, fx=0.2, fy=0.2, interpolation=cv.INTER_AREA)\n",
        "            else:\n",
        "                img_1 = img_0\n",
        "\n",
        "            # plt.subplot(122)\n",
        "            # plt.imshow(img_0), plt.title(f'{img_0.shape}')\n",
        "            # plt.show()\n",
        "\n",
        "            prob_map_patch = evaluate_img_patch(model, img_1, train_tfms, img_width, img_height)\n",
        "\n",
        "            #plt.title(f'name={path.stem}. \\n cut-off threshold = {threshold}', fontsize=4)\n",
        "            prob_map_viz_patch = prob_map_patch.copy()\n",
        "            prob_map_viz_patch = prob_map_viz_patch/ prob_map_viz_patch.max()\n",
        "            prob_map_viz_patch[prob_map_viz_patch < threshold] = 0.0\n",
        "            fig = plt.figure()\n",
        "            st = fig.suptitle(f'name={path.stem} \\n cut-off threshold = {threshold}', fontsize=\"x-large\")\n",
        "            ax = fig.add_subplot(231)\n",
        "            ax.imshow(img_1)\n",
        "            ax = fig.add_subplot(232)\n",
        "            ax.imshow(prob_map_viz_patch)\n",
        "            ax = fig.add_subplot(233)\n",
        "            ax.imshow(img_1)\n",
        "            ax.imshow(prob_map_viz_patch, alpha=0.4)\n",
        "\n",
        "            prob_map_viz_full = prob_map_full.copy()\n",
        "            prob_map_viz_full[prob_map_viz_full < threshold] = 0.0\n",
        "\n",
        "            ax = fig.add_subplot(234)\n",
        "            ax.imshow(img_0)\n",
        "            ax = fig.add_subplot(235)\n",
        "            ax.imshow(prob_map_viz_full)\n",
        "            ax = fig.add_subplot(236)\n",
        "            ax.imshow(img_0)\n",
        "            ax.imshow(prob_map_viz_full, alpha=0.4)\n",
        "\n",
        "            plt.savefig(join(out_viz_dir, f'{path.stem}.jpg'), dpi=500)\n",
        "            plt.close('all')\n",
        "\n",
        "        gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWAEjpRiUCc0"
      },
      "source": [
        "# infer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_55Ad2mVJjTm"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, RandomSampler\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import shutil\n",
        "import os\n",
        "import argparse\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import imageio\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def create_model(device, type ='vgg16'):\n",
        "    if type == 'vgg16':\n",
        "        print('create vgg16 model')\n",
        "        model = UNet16(pretrained=True)\n",
        "    elif type == 'resnet101':\n",
        "        encoder_depth = 101\n",
        "        num_classes = 1\n",
        "        print('create resnet101 model')\n",
        "        model = UNetResNet(encoder_depth=encoder_depth, num_classes=num_classes, pretrained=True)\n",
        "    elif type == 'resnet34':\n",
        "        encoder_depth = 34\n",
        "        num_classes = 1\n",
        "        print('create resnet34 model')\n",
        "        model = UNetResNet(encoder_depth=encoder_depth, num_classes=num_classes, pretrained=True)\n",
        "    else:\n",
        "        assert False\n",
        "    model.eval()\n",
        "    return model.to(device)\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, lr):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = lr * (0.1 ** (epoch // 30))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def find_latest_model_path(dir):\n",
        "    model_paths = []\n",
        "    epochs = []\n",
        "    for path in Path(dir).glob('*.pt'):\n",
        "        if 'epoch' not in path.stem:\n",
        "            continue\n",
        "        model_paths.append(path)\n",
        "        parts = path.stem.split('_')\n",
        "        epoch = int(parts[-1])\n",
        "        epochs.append(epoch)\n",
        "\n",
        "    if len(epochs) > 0:\n",
        "        epochs = np.array(epochs)\n",
        "        max_idx = np.argmax(epochs)\n",
        "        return model_paths[max_idx]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, validation, valid_loader, model_dir, n_epoch, lr, adjust_lr, batch_size):\n",
        "\n",
        "    latest_model_path = find_latest_model_path(model_dir)\n",
        "\n",
        "    best_model_path = os.path.join(*[model_dir, 'model_best.pt'])\n",
        "\n",
        "    if latest_model_path is not None:\n",
        "        state = torch.load(latest_model_path)\n",
        "        epoch = state['epoch']\n",
        "        model.load_state_dict(state['model'])\n",
        "        epoch = epoch\n",
        "\n",
        "        #if latest model path does exist, best_model_path should exists as well\n",
        "        assert Path(best_model_path).exists() == True, f'best model path {best_model_path} does not exist'\n",
        "        #load the min loss so far\n",
        "        best_state = torch.load(latest_model_path)\n",
        "        min_val_los = best_state['valid_loss']\n",
        "\n",
        "        print(f'Restored model at epoch {epoch}. Min validation loss so far is : {min_val_los}')\n",
        "        epoch += 1\n",
        "        print(f'Started training model from epoch {epoch}')\n",
        "    else:\n",
        "        print('Started training model from epoch 0')\n",
        "        epoch = 0\n",
        "        min_val_los = 9999\n",
        "\n",
        "    valid_losses = []\n",
        "    scaled_lr = 0.5 * lr\n",
        "    for epoch in range(epoch, n_epoch + 1):\n",
        "\n",
        "        if (adjust_lr):\n",
        "          adjust_learning_rate(optimizer, epoch, lr)\n",
        "        else:\n",
        "          lr = scaled_lr\n",
        "\n",
        "        tq = tqdm(total=(len(train_loader) * batch_size))\n",
        "        tq.set_description(f'Epoch {epoch}')\n",
        "\n",
        "        losses = AverageMeter()\n",
        "\n",
        "        model.train()\n",
        "        for i, (input, target) in enumerate(train_loader):\n",
        "            input_var  = Variable(input).cuda()\n",
        "            target_var = Variable(target).cuda()\n",
        "\n",
        "            masks_pred = model(input_var)\n",
        "\n",
        "            masks_probs_flat = masks_pred.view(-1)\n",
        "            true_masks_flat  = target_var.view(-1)\n",
        "\n",
        "            loss = criterion(masks_probs_flat, true_masks_flat)\n",
        "            losses.update(loss)\n",
        "            tq.set_postfix(loss='{:.5f}'.format(losses.avg))\n",
        "            tq.update(batch_size)\n",
        "\n",
        "            # compute gradient and do SGD step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        valid_metrics = validation(model, valid_loader, criterion)\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "          calculate_metrics(model, valid_loader)\n",
        "\n",
        "        valid_loss = valid_metrics['valid_loss']\n",
        "        valid_losses.append(valid_loss)\n",
        "        print(f'\\tvalid_loss = {valid_loss:.5f}')\n",
        "        tq.close()\n",
        "\n",
        "        #save the model of the current epoch\n",
        "        epoch_model_path = os.path.join(*[model_dir, f'model_epoch_{epoch}.pt'])\n",
        "        torch.save({\n",
        "            'model': model.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'valid_loss': valid_loss,\n",
        "            'train_loss': losses.avg\n",
        "        }, epoch_model_path)\n",
        "\n",
        "        if valid_loss < min_val_los:\n",
        "            min_val_los = valid_loss\n",
        "\n",
        "            torch.save({\n",
        "                'model': model.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'valid_loss': valid_loss,\n",
        "                'train_loss': losses.avg\n",
        "            }, best_model_path)\n",
        "\n",
        "def validate(model, val_loader, criterion):\n",
        "    losses = AverageMeter()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            input_var = Variable(input).cuda()\n",
        "            target_var = Variable(target).cuda()\n",
        "\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            losses.update(loss.item(), input_var.size(0))\n",
        "\n",
        "    return {'valid_loss': losses.avg}\n",
        "\n",
        "def calculate_metrics(model, val_loader):\n",
        "  \n",
        "  precisions = torch.zeros(len(val_loader))\n",
        "  recalls = torch.zeros(len(val_loader))\n",
        "  f1s = torch.zeros(len(val_loader))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for k, (input, target) in enumerate(tqdm(val_loader)):\n",
        "        target_var = Variable(target).cuda()\n",
        "        input_var = Variable(input).cuda()\n",
        "\n",
        "        output = model(input_var)\n",
        "\n",
        "        target = target_var.cpu()[0,0]\n",
        "\n",
        "        # padded_target = F.pad(input=target, pad=(1, 1, 1, 1), mode='constant', value=0)\n",
        "\n",
        "        # target_transitions = padded_target.clone()\n",
        "\n",
        "        # for i in range(1, padded_target.shape[0]-1):\n",
        "        #   for j in range(1, padded_target.shape[1]-1):\n",
        "        #     neighbours1 = padded_target[i-1][j-1] + padded_target[i-1][j] + padded_target[i-1][j+1]\n",
        "        #     neighbours2 = padded_target[i][j-1] + padded_target[i][j+1]\n",
        "        #     neighbours3 = padded_target[i+1][j-1] + padded_target[i+1][j] + padded_target[i+1][j+1]\n",
        "\n",
        "        #     if (neighbours1 + neighbours2 + neighbours3 > 0):\n",
        "        #       target_transitions[i][j] = 1\n",
        "\n",
        "        # target = target_transitions[1:target_transitions.shape[0]-1, 1:target_transitions.shape[1]-1]\n",
        "        target = target.flatten()\n",
        "        target[target>=0.5] = 1\n",
        "        target[target<0.5] = 0\n",
        "\n",
        "        output = F.sigmoid(output).cpu()[0,0].flatten()\n",
        "        output[output>=0.5] = 1\n",
        "        output[output<0.5] = 0\n",
        "\n",
        "        \n",
        "        precisions[k] = precision_score(target, output, zero_division=1)\n",
        "        recalls[k] = recall_score(target, output, zero_division = 1)\n",
        "        f1s[k] = f1_score(target, output, zero_division=1)\n",
        "  \n",
        "  print(\"precision: \", torch.mean(precisions))\n",
        "  print(\"recall: \", torch.mean(recalls))\n",
        "  print(\"f1 \", torch.mean(f1s))            \n",
        "\n",
        "def save_check_point(state, is_best, file_name = 'checkpoint.pth.tar'):\n",
        "    torch.save(state, file_name)\n",
        "    if is_best:\n",
        "        shutil.copy(file_name, 'model_best.pth.tar')\n",
        "\n",
        "def calc_crack_pixel_weight(mask_dir):\n",
        "    avg_w = 0.0\n",
        "    n_files = 0\n",
        "    for path in Path(mask_dir).glob('*.*'):\n",
        "        n_files += 1\n",
        "        # m = ndimage.imread(path)\n",
        "        m = imageio.imread(path)\n",
        "        ncrack = np.sum((m > 0)[:])\n",
        "        w = float(ncrack)/(m.shape[0]*m.shape[1])\n",
        "        avg_w = avg_w + (1-w)\n",
        "\n",
        "    avg_w /= float(n_files)\n",
        "\n",
        "    return avg_w / (1.0 - avg_w)\n",
        "\n",
        "def train_all():\n",
        "    \n",
        "    # parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
        "    # parser.add_argument('-n_epoch', default=10, type=int, metavar='N', help='number of total epochs to run')\n",
        "    # parser.add_argument('-lr', default=0.001, type=float, metavar='LR', help='initial learning rate')\n",
        "    # parser.add_argument('-momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
        "    # parser.add_argument('-print_freq', default=20, type=int, metavar='N', help='print frequency (default: 10)')\n",
        "    # parser.add_argument('-weight_decay', default=1e-4, type=float, metavar='W', help='weight decay (default: 1e-4)')\n",
        "    # parser.add_argument('-batch_size',  default=4, type=int,  help='weight decay (default: 1e-4)')\n",
        "    # parser.add_argument('-num_workers', default=4, type=int, help='output dataset directory')\n",
        "\n",
        "    # parser.add_argument('-data_dir',type=str, help='input dataset directory')\n",
        "    # parser.add_argument('-model_dir', type=str, help='output dataset directory')\n",
        "    # parser.add_argument('-model_type', type=str, required=False, default='resnet101', choices=['vgg16', 'resnet101', 'resnet34'])\n",
        "\n",
        "    n_epoch = 50\n",
        "    lr = 0.001\n",
        "    momentum = 0.9\n",
        "    print_freq = 20\n",
        "    weight_decay = 1e-4\n",
        "    batch_size = 4\n",
        "    num_workers = 4\n",
        "\n",
        "    # Ablation study parameters\n",
        "    adjust_lr = False\n",
        "    scale_imgs = True\n",
        "\n",
        "    data_dir = 'drive/MyDrive/synced'\n",
        "    model_dir = 'drive/MyDrive/synced/models'\n",
        "    model_type = 'resnet34'\n",
        "\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    DIR_IMG  = os.path.join(data_dir, 'images')\n",
        "    DIR_MASK = os.path.join(data_dir, 'masks')\n",
        "\n",
        "    patterns = ('CFD*.jpg', 'CRACK500*.jpg')\n",
        "    #patterns = ('*.jpg', 'abcdefgh')\n",
        "    imgs = []\n",
        "    masks = []\n",
        "    for pattern in patterns:\n",
        "      imgs.extend(Path(DIR_IMG).glob(pattern))\n",
        "      masks.extend(Path(DIR_MASK).glob(pattern))\n",
        "    img_names  = [path.name for path in imgs]\n",
        "    mask_names = [path.name for path in masks]\n",
        "\n",
        "    print(f'total images = {len(img_names)}')\n",
        "\n",
        "    print(f'cuda available: {torch.cuda.is_available()}')\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = create_model(device, model_type)\n",
        "\n",
        "    # optimizer = torch.optim.SGD(model.parameters(), lr,\n",
        "    #                             momentum=momentum,\n",
        "    #                             weight_decay=weight_decay)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr, weight_decay = weight_decay)\n",
        "\n",
        "    # crack_weight = 0.4*calc_crack_pixel_weight(DIR_MASK)\n",
        "    # print(f'positive weight: {crack_weight}')\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([10])).to('cuda')\n",
        "    criterion = nn.BCEWithLogitsLoss().to('cuda')\n",
        "\n",
        "    channel_means = [0.485, 0.456, 0.406]\n",
        "    channel_stds  = [0.229, 0.224, 0.225]\n",
        "    \n",
        "    if (scale_imgs):\n",
        "      train_tfms = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(channel_means, channel_stds),\n",
        "                                     transforms.Resize(224)])\n",
        "      \n",
        "      mask_tfms = transforms.Compose([transforms.ToTensor(), transforms.Resize(224)])\n",
        "\n",
        "      \n",
        "    else:\n",
        "      train_tfms = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(channel_means, channel_stds)])\n",
        "      \n",
        "      mask_tfms = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "    \n",
        "    val_tfms = transforms.Compose([transforms.ToTensor(),\n",
        "                                   transforms.Normalize(channel_means, channel_stds)])\n",
        "\n",
        "\n",
        "    dataset = ImgDataSet(img_dir=DIR_IMG, img_fnames=img_names, img_transform=train_tfms, mask_dir=DIR_MASK, mask_fnames=mask_names, mask_transform=mask_tfms)\n",
        "    dataset = torch.utils.data.Subset(dataset, range(int(0.5 * len(dataset))))\n",
        "    train_size = int(0.85*len(dataset))\n",
        "    valid_size = len(dataset) - train_size\n",
        "    train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size, shuffle=False, pin_memory=torch.cuda.is_available(), num_workers=num_workers)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size, shuffle=False, pin_memory=torch.cuda.is_available(), num_workers=num_workers)\n",
        "\n",
        "    model.cuda()\n",
        "\n",
        "    train(train_loader, model, criterion, optimizer, validate, valid_loader, model_dir, n_epoch, lr, adjust_lr, batch_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwGjWYzzEwB2"
      },
      "source": [
        "# Run validation without training\n",
        "\n",
        "model_type = 'resnet34'\n",
        "model_dir = 'drive/MyDrive/synced/models'\n",
        "\n",
        "def get_valid_loader():\n",
        "  data_dir = 'drive/MyDrive/synced'\n",
        "\n",
        "  DIR_IMG  = os.path.join(data_dir, 'images')\n",
        "  DIR_MASK = os.path.join(data_dir, 'masks')\n",
        "\n",
        "  patterns = ('CFD*.jpg', 'CRACK500*.jpg')\n",
        "  imgs = []\n",
        "  masks = []\n",
        "  for pattern in patterns:\n",
        "    imgs.extend(Path(DIR_IMG).glob(pattern))\n",
        "    masks.extend(Path(DIR_MASK).glob(pattern))\n",
        "  img_names  = [path.name for path in imgs]\n",
        "  mask_names = [path.name for path in masks]\n",
        "\n",
        "  channel_means = [0.485, 0.456, 0.406]\n",
        "  channel_stds  = [0.229, 0.224, 0.225]\n",
        "\n",
        "  train_tfms = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize(channel_means, channel_stds)])\n",
        "\n",
        "\n",
        "  val_tfms = transforms.Compose([transforms.ToTensor(),\n",
        "                                  transforms.Normalize(channel_means, channel_stds)])\n",
        "\n",
        "  mask_tfms = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "  dataset = ImgDataSet(img_dir=DIR_IMG, img_fnames=img_names, img_transform=train_tfms, mask_dir=DIR_MASK, mask_fnames=mask_names, mask_transform=mask_tfms)\n",
        "  dataset = torch.utils.data.Subset(dataset, range(int(0.5 * len(dataset))))\n",
        "  train_size = int(0.85*len(dataset))\n",
        "  valid_size = len(dataset) - train_size\n",
        "  _, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "\n",
        "  return DataLoader(valid_dataset, 4, shuffle=False, pin_memory=torch.cuda.is_available(), num_workers=4)\n",
        "\n",
        "def get_model():\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = create_model(device, model_type)\n",
        "\n",
        "  best_model_path = os.path.join(*[model_dir, 'model_best.pt'])\n",
        "\n",
        "  state = torch.load(best_model_path)\n",
        "  model.load_state_dict(state['model']) \n",
        "  \n",
        "  return model\n",
        "\n",
        "# calculate_metrics(get_model(), get_valid_loader())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPEp8tk-Ca5c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495,
          "referenced_widgets": [
            "332075e5de88426da4336b5143c92608",
            "dd9efdb3b96540c29930f10a99d1a465",
            "252d3ade33124ba9a7be1a8d5a3dcc57",
            "0f53cc85a3c0444eb1c9afe3a32f4690",
            "c9922fc5d11747a5956c857d5f1fb609",
            "caf24766b9044933a4baeafabe2a8b85",
            "0f19155e53cf49c7a59ac29be5778ad0",
            "2c1a6a15d72540da9936608d9c520e6e"
          ]
        },
        "id": "f6L0nBcmlWyV",
        "outputId": "a1054253-2f2e-4c36-f271-1cd66f83896f"
      },
      "source": [
        "train_all()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total images = 2449\n",
            "cuda available: True\n",
            "create resnet34 model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Restored model at epoch 0. Min validation loss so far is : 0.6900064676352169\n",
            "Started training model from epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "332075e5de88426da4336b5143c92608",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1040.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-6d37eaef03f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-5478e0778e2c>\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m()\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjust_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-5478e0778e2c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, validation, valid_loader, model_dir, n_epoch, lr, adjust_lr, batch_size)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mvalid_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                     group['eps'])\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn8JefaTUZ_2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT4txLo7mJUt"
      },
      "source": [
        "print(\"hello net\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQnWLqpO0jSM"
      },
      "source": [
        "# !rm -rf drive/MyDrive/synced/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUjOmuYQxymu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAZ-gycbCfsJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}